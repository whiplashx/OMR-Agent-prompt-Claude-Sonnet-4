# OMR (Optical Music Recognition) Pipeline

A comprehensive, end-to-end Optical Music Recognition system that converts sheet music images into validated MusicXML files compatible with MuseScore.

## ğŸ¼ Features

### Core Functionality
- **Image Preprocessing**: Advanced noise reduction, binarization, skew correction, and contrast enhancement
- **Staff Detection**: Precise Hough line detection with careful staff removal preserving musical symbols
- **Symbol Detection**: YOLO-based instance segmentation and classification of 50+ musical symbol types
- **Music Reconstruction**: Intelligent pitch calculation, voice separation, and measure grouping
- **MusicXML Output**: MuseScore-compatible MusicXML generation with divisions=480 for precise timing
- **Confidence Analysis**: Comprehensive JSON reports with confidence scores and quality assessment

### Advanced Features
- **Manual Correction UI**: Interactive Streamlit interface with plotly visualization for result editing
- **Dataset Management**: Synthetic data generation, augmentation pipeline, and IMSLP dataset support
- **Evaluation Metrics**: Symbol-level and semantic-level evaluation with comprehensive benchmarking
- **Batch Processing**: Efficient processing of multiple images with detailed progress tracking
- **Quality Assessment**: Automated quality scoring and improvement recommendations

## ğŸ“ Project Structure

```
d:\OMR\
â”œâ”€â”€ src\
â”‚   â”œâ”€â”€ preprocessing\
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ image_preprocessor.py      # Image preprocessing pipeline
â”‚   â”œâ”€â”€ detection\
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ staff_detector.py          # Staff line detection & removal
â”‚   â”‚   â””â”€â”€ symbol_detector.py         # YOLO-based symbol detection
â”‚   â”œâ”€â”€ reconstruction\
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ music_reconstructor.py     # Musical meaning reconstruction
â”‚   â”œâ”€â”€ output\
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ musicxml_generator.py      # MusicXML generation
â”‚   â”‚   â””â”€â”€ json_exporter.py           # JSON confidence reports
â”‚   â”œâ”€â”€ ui\
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ correction_interface.py    # Manual correction interface
â”‚   â”œâ”€â”€ data\
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ dataset_manager.py         # Dataset management & augmentation
â”‚   â”œâ”€â”€ evaluation\
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py                 # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ evaluation_cli.py          # Evaluation command-line tool
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ omr_pipeline.py                # Main pipeline orchestrator
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.py                          # Package setup
â”œâ”€â”€ README.md                          # This file
â””â”€â”€ examples\
    â”œâ”€â”€ basic_usage.py                 # Basic usage examples
    â”œâ”€â”€ batch_processing.py            # Batch processing examples
    â”œâ”€â”€ manual_correction_demo.py      # Manual correction demo
    â””â”€â”€ evaluation_examples.py         # Evaluation examples
```

## ğŸš€ Quick Start

### Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd OMR
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Download pre-trained models** (if available):
```bash
# YOLO model for symbol detection
mkdir -p models
# Download your trained YOLO model to models/symbol_detector.pt
```

### Basic Usage

```python
from src.omr_pipeline import OMRPipeline
import cv2

# Initialize the pipeline
pipeline = OMRPipeline()

# Load and process an image
image = cv2.imread('path/to/sheet_music.png')
results = pipeline.process_image(image, 'path/to/sheet_music.png')

if results.success:
    # Save MusicXML output
    pipeline.save_musicxml(results.musicxml_content, 'output.mxl')
    
    # Save JSON confidence report
    pipeline.save_json_report(results.json_report, 'output_report.json')
    
    print(f"Processing completed in {results.total_time:.2f}s")
    print(f"Overall confidence: {results.confidence_scores['overall_confidence']:.3f}")
else:
    print(f"Processing failed: {results.error_message}")
```

### Command Line Usage

```bash
# Process a single image
python -m src.omr_pipeline input_image.png output_directory

# Batch processing
python -m src.omr_pipeline input_directory output_directory --batch --intermediate

# With custom configuration
python -m src.omr_pipeline input.png output --config config.json --verbose
```

### Configuration File Example

```json
{
  "preprocessing": {
    "denoise_strength": 5,
    "contrast_enhancement": true,
    "skew_correction": true,
    "binarization_method": "adaptive"
  },
  "staff_detection": {
    "line_thickness_range": [1, 5],
    "min_line_length": 100,
    "angle_tolerance": 2.0
  },
  "symbol_detection": {
    "confidence_threshold": 0.3,
    "nms_threshold": 0.4,
    "model_size": "medium"
  },
  "music_reconstruction": {
    "voice_separation_threshold": 30,
    "measure_grouping": true,
    "rhythm_quantization": true
  },
  "musicxml_generation": {
    "divisions": 480,
    "validate_output": true,
    "include_layout": false
  }
}
```

## ğŸ› ï¸ Manual Correction Interface

Launch the interactive correction interface:

```python
from src.ui.correction_interface import OMRCorrectionUI

# Initialize and run the interface
ui = OMRCorrectionUI()
ui.run()
```

Then open your browser to `http://localhost:8501` to access the interface.

### Interface Features:
- **Visual Inspection**: Interactive image display with detected symbols highlighted
- **Click-to-Edit**: Click on symbols to edit their classification or properties
- **Confidence Filtering**: Filter symbols by confidence level for focused correction
- **Export Options**: Save corrected results as MusicXML or JSON

## ğŸ“Š Evaluation and Metrics

### Run Evaluation on a Dataset

```bash
python -m src.evaluation.evaluation_cli dataset_path output_dir --config eval_config.json
```

### Programmatic Evaluation

```python
from src.evaluation import OMREvaluator

evaluator = OMREvaluator()
results = evaluator.evaluate_full_pipeline(predicted_results, ground_truth)
report = evaluator.generate_evaluation_report(results)

print(f"Overall accuracy: {report['summary']['overall_average']:.3f}")
```

### Available Metrics:
- **Symbol-level**: Detection precision/recall, classification accuracy, confidence calibration
- **Semantic-level**: Pitch accuracy, rhythm accuracy, key/time signature detection
- **Musical similarity**: Composite scoring for overall musical correctness

## ğŸ—ƒï¸ Dataset Management

### Synthetic Data Generation

```python
from src.data.dataset_manager import DatasetManager

dataset_manager = DatasetManager('path/to/dataset')

# Generate synthetic training data
dataset_manager.generate_synthetic_data(
    num_images=1000,
    output_dir='synthetic_data',
    include_augmentation=True
)

# Export for YOLO training
dataset_manager.export_yolo_format('yolo_dataset')
```

### Data Augmentation

```python
from src.data.dataset_manager import DataAugmentation

augmenter = DataAugmentation()
augmented_image, augmented_annotations = augmenter.augment(image, annotations)
```

## ğŸ¯ Symbol Classes

The system recognizes 50+ musical symbol types including:

**Notes**: whole_note, half_note, quarter_note, eighth_note, sixteenth_note
**Rests**: whole_rest, half_rest, quarter_rest, eighth_rest, sixteenth_rest
**Clefs**: treble_clef, bass_clef, alto_clef, tenor_clef
**Key Signatures**: sharp, flat, natural
**Time Signatures**: common_time, cut_time, plus numeric signatures
**Accidentals**: sharp, flat, natural, double_sharp, double_flat
**Articulations**: staccato, accent, tenuto, fermata
**Dynamics**: piano, forte, crescendo, diminuendo
**And many more...**

## ğŸ“ˆ Performance

### Typical Processing Times (on modern CPU):
- **Small image** (800x600): ~2-5 seconds
- **Medium image** (1200x900): ~5-10 seconds  
- **Large image** (2400x1800): ~10-20 seconds

### Accuracy Expectations:
- **Staff Detection**: >95% accuracy on clean printed music
- **Symbol Detection**: >85% F1-score with proper training data
- **Pitch Accuracy**: >90% for clearly printed notes
- **Overall Pipeline**: >80% end-to-end accuracy on quality sheet music

## ğŸ”§ Development and Customization

### Adding New Symbol Classes

1. Update `SYMBOL_CLASSES` in `src/detection/symbol_detector.py`
2. Retrain YOLO model with new symbol data
3. Update reconstruction logic in `src/reconstruction/music_reconstructor.py`

### Custom Preprocessing

```python
from src.preprocessing.image_preprocessor import ImagePreprocessor

class CustomPreprocessor(ImagePreprocessor):
    def custom_enhancement(self, image):
        # Your custom enhancement logic
        return enhanced_image
    
    def process(self, image):
        # Call parent process method
        processed = super().process(image)
        # Apply custom enhancement
        return self.custom_enhancement(processed)
```

### Extending Evaluation Metrics

```python
from src.evaluation.metrics import SemanticLevelEvaluator

class CustomEvaluator(SemanticLevelEvaluator):
    def evaluate_harmony_accuracy(self, predicted, ground_truth):
        # Custom harmony evaluation logic
        pass
```

## ğŸ› Troubleshooting

### Common Issues

1. **Low detection accuracy**: 
   - Check image quality and resolution
   - Adjust confidence thresholds in configuration
   - Ensure proper training data for your music style

2. **Staff detection failures**:
   - Verify image skew correction is enabled
   - Adjust line thickness parameters
   - Check for non-standard staff spacing

3. **Memory issues with large images**:
   - Reduce image resolution in preprocessing
   - Process images in smaller batches
   - Increase system memory allocation

4. **MusicXML validation errors**:
   - Check symbol-to-musical-element mapping
   - Verify measure grouping logic
   - Enable MusicXML validation in configuration

### Debug Mode

Enable verbose logging for detailed debugging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

pipeline = OMRPipeline()
results = pipeline.process_image(image)
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and add tests
4. Run the evaluation suite: `python -m src.evaluation.evaluation_cli test_dataset output`
5. Submit a pull request

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
python -m pytest tests/

# Check code formatting
black src/
flake8 src/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“š References

- **MusicXML Specification**: https://www.musicxml.com/
- **YOLO Documentation**: https://docs.ultralytics.com/
- **MuseScore**: https://musescore.org/
- **OpenCV Documentation**: https://docs.opencv.org/

## ğŸµ Acknowledgments

This OMR system builds upon research in computer vision, music information retrieval, and optical character recognition. Special thanks to the open-source community for providing the foundational tools and libraries that make this project possible.

---

**Note**: This is a research-grade system designed for educational and research purposes. For production use, additional testing, validation, and optimization may be required depending on your specific use case and music notation requirements.